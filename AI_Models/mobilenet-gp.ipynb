{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8479214,"sourceType":"datasetVersion","datasetId":5057198}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" import os\n import shutil\n\n # Input folder containing the images\n input_dir = r\"/kaggle/input/skin-disease-dataset/dataset/train\"\n # Output folder for renamed images\n output_dir = r\"/kaggle/working/renamed_train\"\n\n # Ensure the output directory exists\n os.makedirs(output_dir, exist_ok=True)\n\n # Dictionary to track counts for each class\n class_counts = {}\n\n # Traverse through each subdirectory\n for root, dirs, files in os.walk(input_dir):\n     for file_name in files:\n         # Full path of the image\n         img_path = os.path.join(root, file_name)\n\n         # Skip non-image files\n         if not file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n             print(f\"Skipping non-image file: {file_name}\")\n             continue\n\n         # Get the folder name (class name) as the class identifier\n         class_name = os.path.basename(root)\n\n         # Initialize or increment the count for this class\n         if class_name not in class_counts:\n             class_counts[class_name] = 1\n         else:\n             class_counts[class_name] += 1\n\n         # Generate new file name in the format ClassName(Count).Extension\n         count = class_counts[class_name]\n         ext = os.path.splitext(file_name)[1]  # Get file extension\n         new_name = f\"{class_name}({count}){ext}\"\n         new_path = os.path.join(output_dir, new_name)\n\n         # Copy and rename the file to the output directory\n         shutil.copy(img_path, new_path)\n\n # Print the total number of images for each class\n print(\"\\nImage counts by class:\")\n for class_name, count in class_counts.items():\n     print(f\"{class_name}: {count} images\")\n\n print(\"\\nRenaming and consolidation complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:48:01.028089Z","iopub.execute_input":"2025-04-21T22:48:01.028371Z","iopub.status.idle":"2025-04-21T22:48:13.241285Z","shell.execute_reply.started":"2025-04-21T22:48:01.028347Z","shell.execute_reply":"2025-04-21T22:48:13.240495Z"}},"outputs":[{"name":"stdout","text":"\nImage counts by class:\nEczema: 999 images\nMelanoma: 1000 images\nBasal Cell: 1000 images\nSeborrheic: 1000 images\nAtopic Dermatitis: 1000 images\nMelanocytic: 1000 images\nBenign Keratosis: 1201 images\nWarts Molluscum: 1000 images\nPsoriasis: 1000 images\nTinea Ringworms Candidiasis: 990 images\n\nRenaming and consolidation complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"'''import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNet  # Import MobileNet\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\n# Define class mapping (updated with \"Basal Cell\")\nclass_mapping = {\n    \"Seborrheic\": 0,\n    \"Melanocytic\": 1,\n    \"Melanoma\": 2,\n    \"Eczema\": 3,\n    \"Basal_Cell\": 4,\n}\n\n# Preprocess images: resize and normalize\ndef preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Warning: {image_path} could not be loaded.\")\n        return None\n\n    resized_image = cv2.resize(image, (224, 224))  # Resizing to 224x224\n    img_normalized = resized_image.astype('float32') / 255.0  # Normalize to [0, 1]\n    return img_normalized\n\ndef load_data_from_single_folder(folder):\n    images = []\n    labels = []\n\n    for image_name in os.listdir(folder):\n        image_path = os.path.join(folder, image_name)\n\n        # Check if file is an image\n        if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n            # Extract the label from the filename (before the parentheses)\n            label = image_name.split('(')[0].strip().replace(' ', '_')  # Handle spaces and extract class name\n            \n            if label in class_mapping:\n                label_index = class_mapping[label]  # Map label to integer\n            else:\n                #print(f\"Warning: Label {label} not found in mapping. Skipping image.\")\n                continue\n\n            # Preprocess the image\n            preprocessed_image = preprocess_image(image_path)\n            if preprocessed_image is not None:\n                images.append(preprocessed_image)\n                labels.append(label_index)\n\n    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n    return np.array(images), np.array(labels)\n\n# Paths for train folder\ntrain_folder = r'/kaggle/working/renamed_train'\n\n# Load data\nX_train, y_train = load_data_from_single_folder(train_folder)\n\n# === Step 6: Class Distribution Analysis ===\nclass_counts = pd.Series(y_train).value_counts()\nclass_names = {v: k for k, v in class_mapping.items()}  # Reverse the mapping\nclass_counts_named = class_counts.rename(index=class_names)\n\nprint(\"\\nClass counts (class names):\")\nprint(class_counts_named)\n\n# === Step 7: Balance Classes to Max Class Size Using Augmentation ===\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nmax_class_size = class_counts.max()  # Maximum size among all classes\naugmented_images = []\naugmented_labels = []\n\n# Create augmented images for each class\nfor label in np.unique(y_train):\n    class_images = X_train[y_train == label]\n    current_class_size = class_counts[label]\n\n    augmented = datagen.flow(class_images, batch_size=1)\n    for _ in range(max_class_size - current_class_size):\n        augmented_images.append(next(augmented)[0])\n        augmented_labels.append(label)\n\n# If augmented images are created, concatenate them with the original data\nif augmented_images:  # Ensure there are augmented images to add\n    X_train = np.concatenate([X_train, np.array(augmented_images)])\n    y_train = np.concatenate([y_train, np.array(augmented_labels)])\n\n# Check new class distribution\nnew_class_counts = pd.Series(y_train).value_counts()\nnew_class_counts_named = new_class_counts.rename(index=class_names)\n\nprint(\"\\nNew class counts after augmentation (class names):\")\nprint(new_class_counts_named)\n\n# Split the dataset into training and validation sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n# Check new class distribution for train and validation sets\ntrain_class_counts = pd.Series(y_train).value_counts().rename(index=class_names)\ntest_class_counts = pd.Series(y_test).value_counts().rename(index=class_names)\n\nprint(\"\\nClass counts in training set:\")\nprint(train_class_counts)\n\nprint(\"\\nClass counts in test set:\")\nprint(test_class_counts)\n\n# === Replace VGG16 with MobileNet Pretrained Model ===\n# Load MobileNet model with pretrained weights, excluding the top layers\nbase_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the base model layers to prevent them from being trained\nbase_model.trainable = False\n\n# Create a new model on top of MobileNet\nmodel = models.Sequential()\n\n# Add the MobileNet base model\nmodel.add(base_model)\n\n# Add custom fully connected layers\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(1024, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5))  #1 Regularize the fully connected layer\nmodel.add(layers.Dense(1024, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5)) #2 Regularize the fully connected layer\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(len(class_mapping), activation='softmax'))  # Output layer updated to 5 classes\n\nfor layer in base_model.layers[-10:]:  # Unfreeze the last 10 layers\n    layer.trainable = True\n    \n# Compile the model with Adam optimizer\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n              loss='sparse_categorical_crossentropy',  # Sparse because labels are integers\n              metrics=['accuracy'])\n\n# Learning rate scheduler\ncallbacks = [\nEarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\nReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5,min_lr=1e-7,verbose=1)\n]\n\n# Train the model with validation data\nepochs = 50\nbatch_size = 16\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n                    validation_data=(X_test, y_test), callbacks=callbacks)\n\n# Predict on the validation data\ny_test_pred = model.predict(X_test)\ny_test_pred_classes = np.argmax(y_test_pred, axis=1)\n\n# Confusion matrix for validation set\ntest_conf_matrix = confusion_matrix(y_test, y_test_pred_classes)\nprint(f\"Test Confusion Matrix:\\n{test_conf_matrix}\")\n\n# Calculate overall accuracy for the validation set\ntest_accuracy = np.mean(y_test_pred_classes == y_test)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n# Print the first 10 predictions and actual class names from validation set\nlabel_to_class = {v: k for k, v in class_mapping.items()}\n\nfor i in range(10):\n    predicted_class = label_to_class[y_test_pred_classes[i]]\n    actual_class = label_to_class[y_test[i]]\n    print(f\"Predicted: {predicted_class}, Actual: {actual_class}\")'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport math\nfrom collections import Counter\nfrom torchvision.models import mobilenet_v3_large\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Class mapping and parameters\nclass_mapping = {\n    \"Seborrheic\": 0, \"Melanocytic\": 1, \"Melanoma\": 2, \"Eczema\": 3,\n    \"Basal_Cell\": 4, \"Psoriasis\": 5, \"Tinea_Ringworms_Candidiasis\": 6,\n    \"Warts_Molluscum\": 7, \"Atopic_Dermatitis\": 8\n}\nTARGET_SAMPLES_PER_CLASS = 1500\n\n# ================== DATA BALANCING ==================\ndef load_and_balance_data(folder):\n    \"\"\"Load images and ensure exactly 1500 per class using augmentation\"\"\"\n    class_counts = {cls: 0 for cls in class_mapping.values()}\n    images = []\n    labels = []\n    \n    # First pass: collect natural images\n    raw_data = {cls: [] for cls in class_mapping.values()}\n    for img_name in os.listdir(folder):\n        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n            label_name = img_name.split('(')[0].strip().replace(' ', '_')\n            if label_name in class_mapping:\n                label = class_mapping[label_name]\n                img_path = os.path.join(folder, img_name)\n                img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n                if img is not None:\n                    raw_data[label].append(img)\n    \n    # Second pass: balance classes\n    augmenter = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.3),\n        A.Rotate(limit=25),\n        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    ])\n    \n    for cls, cls_images in raw_data.items():\n        # Add original images\n        for img in cls_images[:min(len(cls_images), TARGET_SAMPLES_PER_CLASS)]:\n            images.append(img)\n            labels.append(cls)\n            class_counts[cls] += 1\n        \n        # Augment to reach target count\n        while class_counts[cls] < TARGET_SAMPLES_PER_CLASS:\n            for img in cls_images:\n                if class_counts[cls] >= TARGET_SAMPLES_PER_CLASS:\n                    break\n                augmented = augmenter(image=img)['image']\n                images.append(augmented)\n                labels.append(cls)\n                class_counts[cls] += 1\n    \n    # Verify balancing\n    print(\"\\nFinal class distribution:\")\n    for cls, count in Counter(labels).items():\n        print(f\"Class {cls}: {count} images\")\n    \n    return images, np.array(labels)\n\n# ================== MOBILENET V3 MODEL ==================\nclass MobileNetV3(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.base = mobilenet_v3_large(pretrained=True)\n        \n        # Freeze early layers\n        for param in self.base.features[:10].parameters():\n            param.requires_grad = False\n            \n        # Modify classifier\n        self.base.classifier = nn.Sequential(\n            nn.Linear(960, 1280),\n            nn.Hardswish(inplace=True),\n            nn.Dropout(p=0.2, inplace=True),\n            nn.Linear(1280, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.base(x)\n\n# ================== AUGMENTATIONS ==================\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.RandomCrop(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.3),\n    A.Rotate(limit=25),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    A.CoarseDropout(max_holes=6, max_height=32, max_width=32, p=0.3),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# ================== TRAINING LOOP ==================\ndef train_model():\n    # Load and balance data\n    images, labels = load_and_balance_data(\"/kaggle/working/renamed_train\")\n    X_train, X_val, y_train, y_val = train_test_split(\n        images, labels, test_size=0.2, stratify=labels, random_state=42\n    )\n    \n    # Create datasets\n    class SkinDataset(Dataset):\n        def __init__(self, images, labels, transform=None):\n            self.images = images\n            self.labels = labels\n            self.transform = transform\n        \n        def __len__(self):\n            return len(self.images)\n        \n        def __getitem__(self, idx):\n            img = self.images[idx]\n            label = self.labels[idx]\n            if self.transform:\n                img = self.transform(image=img)['image']\n            return img, label\n    \n    train_dataset = SkinDataset(X_train, y_train, train_transform)\n    val_dataset = SkinDataset(X_val, y_val, val_transform)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n    \n    # Initialize model\n    model = MobileNetV3(len(class_mapping)).to(device)\n    \n    # Weighted loss (adjust weights based on your dataset)\n    class_weights = torch.tensor([1.0, 1.2, 1.5, 1.0, 1.3, 1.0, 1.1, 1.0, 1.2]).to(device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    \n    # Optimizer and scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)\n    \n    # Training\n    best_val_acc = 0.0\n    for epoch in range(30):\n        model.train()\n        train_loss, correct, total = 0, 0, 0\n        \n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/30\"):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100 * correct / total\n        val_loss, val_acc = validate(model, val_loader, criterion)\n        \n        # Apply accuracy ceiling\n        val_acc_ceil = math.ceil(val_acc * 100) / 100\n        \n        # Update scheduler\n        scheduler.step(val_acc)\n        \n        print(f\"Train Loss: {train_loss/len(train_loader):.4f} | Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% (Ceiled: {val_acc_ceil:.2f}%)\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            #torch.save(model.state_dict(), \"best_mobilenetv3_model.pth\")\n            #print(f\"New best model saved (Val Acc: {math.ceil(best_val_acc*100)/100:.2f}%)\")\n\ndef validate(model, val_loader, criterion):\n    model.eval()\n    val_loss, correct, total = 0, 0, 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return val_loss/len(val_loader), 100*correct/total\n\nif __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T22:48:20.713709Z","iopub.execute_input":"2025-04-21T22:48:20.714009Z","iopub.status.idle":"2025-04-21T23:00:31.023774Z","shell.execute_reply.started":"2025-04-21T22:48:20.713990Z","shell.execute_reply":"2025-04-21T23:00:31.022742Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"},{"name":"stdout","text":"\nFinal class distribution:\nClass 0: 1500 images\nClass 1: 1500 images\nClass 2: 1500 images\nClass 3: 1500 images\nClass 4: 1500 images\nClass 5: 1500 images\nClass 6: 1500 images\nClass 7: 1500 images\nClass 8: 1500 images\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nEpoch 1/30: 100%|██████████| 169/169 [00:17<00:00,  9.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9460 | Acc: 62.03%\nVal Loss: 0.8495 | Acc: 67.00% (Ceiled: 67.00%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 169/169 [00:18<00:00,  9.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6919 | Acc: 72.57%\nVal Loss: 0.6825 | Acc: 73.96% (Ceiled: 73.97%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 169/169 [00:18<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5693 | Acc: 77.40%\nVal Loss: 0.6367 | Acc: 77.19% (Ceiled: 77.19%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 169/169 [00:18<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5087 | Acc: 80.02%\nVal Loss: 0.5737 | Acc: 79.41% (Ceiled: 79.41%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 169/169 [00:17<00:00,  9.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4520 | Acc: 82.49%\nVal Loss: 0.5120 | Acc: 81.74% (Ceiled: 81.75%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 169/169 [00:18<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3960 | Acc: 84.70%\nVal Loss: 0.5996 | Acc: 79.41% (Ceiled: 79.41%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 169/169 [00:18<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3500 | Acc: 86.74%\nVal Loss: 0.4585 | Acc: 82.37% (Ceiled: 82.38%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 169/169 [00:18<00:00,  9.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2994 | Acc: 88.66%\nVal Loss: 0.4890 | Acc: 83.22% (Ceiled: 83.23%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 169/169 [00:18<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2886 | Acc: 88.94%\nVal Loss: 0.4829 | Acc: 83.96% (Ceiled: 83.97%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 169/169 [00:18<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2528 | Acc: 90.44%\nVal Loss: 0.4504 | Acc: 85.37% (Ceiled: 85.38%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 169/169 [00:17<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2344 | Acc: 91.17%\nVal Loss: 0.5722 | Acc: 83.63% (Ceiled: 83.63%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 169/169 [00:18<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2288 | Acc: 91.54%\nVal Loss: 0.4436 | Acc: 86.19% (Ceiled: 86.19%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 169/169 [00:18<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2005 | Acc: 92.60%\nVal Loss: 0.4630 | Acc: 85.89% (Ceiled: 85.89%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 169/169 [00:17<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2004 | Acc: 92.71%\nVal Loss: 0.4968 | Acc: 86.81% (Ceiled: 86.82%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 169/169 [00:18<00:00,  8.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1770 | Acc: 93.49%\nVal Loss: 0.4275 | Acc: 86.52% (Ceiled: 86.52%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 169/169 [00:18<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1649 | Acc: 94.17%\nVal Loss: 0.4978 | Acc: 85.70% (Ceiled: 85.71%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 169/169 [00:17<00:00,  9.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1646 | Acc: 93.94%\nVal Loss: 0.4852 | Acc: 86.52% (Ceiled: 86.52%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 169/169 [00:18<00:00,  9.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1557 | Acc: 94.21%\nVal Loss: 0.4446 | Acc: 88.30% (Ceiled: 88.30%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 169/169 [00:18<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1480 | Acc: 94.39%\nVal Loss: 0.4580 | Acc: 87.74% (Ceiled: 87.75%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 169/169 [00:17<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1626 | Acc: 94.19%\nVal Loss: 0.4865 | Acc: 87.22% (Ceiled: 87.23%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 169/169 [00:18<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1165 | Acc: 95.67%\nVal Loss: 0.4628 | Acc: 87.07% (Ceiled: 87.08%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 169/169 [00:18<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1321 | Acc: 95.01%\nVal Loss: 0.4778 | Acc: 87.96% (Ceiled: 87.97%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30: 100%|██████████| 169/169 [00:17<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0780 | Acc: 97.05%\nVal Loss: 0.3992 | Acc: 89.74% (Ceiled: 89.75%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30: 100%|██████████| 169/169 [00:18<00:00,  9.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0491 | Acc: 98.19%\nVal Loss: 0.4408 | Acc: 90.44% (Ceiled: 90.45%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30: 100%|██████████| 169/169 [00:18<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0573 | Acc: 97.90%\nVal Loss: 0.4380 | Acc: 90.41% (Ceiled: 90.41%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30: 100%|██████████| 169/169 [00:17<00:00,  9.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0552 | Acc: 97.95%\nVal Loss: 0.4089 | Acc: 90.70% (Ceiled: 90.71%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30: 100%|██████████| 169/169 [00:18<00:00,  9.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0522 | Acc: 98.10%\nVal Loss: 0.4077 | Acc: 91.15% (Ceiled: 91.15%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30: 100%|██████████| 169/169 [00:18<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0457 | Acc: 98.37%\nVal Loss: 0.4470 | Acc: 90.07% (Ceiled: 90.08%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30: 100%|██████████| 169/169 [00:17<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0468 | Acc: 98.17%\nVal Loss: 0.4389 | Acc: 90.11% (Ceiled: 90.12%)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30: 100%|██████████| 169/169 [00:18<00:00,  9.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0575 | Acc: 97.74%\nVal Loss: 0.4236 | Acc: 90.59% (Ceiled: 90.60%)\n","output_type":"stream"}],"execution_count":2}]}