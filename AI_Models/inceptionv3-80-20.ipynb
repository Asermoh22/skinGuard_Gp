{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8479214,"sourceType":"datasetVersion","datasetId":5057198}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" import os\n import shutil\n\n # Input folder containing the images\n input_dir = r\"/kaggle/input/skin-disease-dataset/dataset/train\"\n # Output folder for renamed images\n output_dir = r\"/kaggle/working/renamed_train\"\n\n # Ensure the output directory exists\n os.makedirs(output_dir, exist_ok=True)\n\n # Dictionary to track counts for each class\n class_counts = {}\n\n # Traverse through each subdirectory\n for root, dirs, files in os.walk(input_dir):\n     for file_name in files:\n         # Full path of the image\n         img_path = os.path.join(root, file_name)\n\n         # Skip non-image files\n         if not file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n             print(f\"Skipping non-image file: {file_name}\")\n             continue\n\n         # Get the folder name (class name) as the class identifier\n         class_name = os.path.basename(root)\n\n         # Initialize or increment the count for this class\n         if class_name not in class_counts:\n             class_counts[class_name] = 1\n         else:\n             class_counts[class_name] += 1\n\n         # Generate new file name in the format ClassName(Count).Extension\n         count = class_counts[class_name]\n         ext = os.path.splitext(file_name)[1]  # Get file extension\n         new_name = f\"{class_name}({count}){ext}\"\n         new_path = os.path.join(output_dir, new_name)\n\n         # Copy and rename the file to the output directory\n         shutil.copy(img_path, new_path)\n\n # Print the total number of images for each class\n print(\"\\nImage counts by class:\")\n for class_name, count in class_counts.items():\n     print(f\"{class_name}: {count} images\")\n\n print(\"\\nRenaming and consolidation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T17:03:58.593535Z","iopub.execute_input":"2025-02-14T17:03:58.594191Z","iopub.status.idle":"2025-02-14T17:04:18.534807Z","shell.execute_reply.started":"2025-02-14T17:03:58.594162Z","shell.execute_reply":"2025-02-14T17:04:18.534022Z"}},"outputs":[{"name":"stdout","text":"\nImage counts by class:\nEczema: 999 images\nMelanoma: 1000 images\nBasal Cell: 1000 images\nSeborrheic: 1000 images\nAtopic Dermatitis: 1000 images\nMelanocytic: 1000 images\nBenign Keratosis: 1201 images\nWarts Molluscum: 1000 images\nPsoriasis: 1000 images\nTinea Ringworms Candidiasis: 990 images\n\nRenaming and consolidation complete!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":" import os\n import shutil\n\n # Input folder containing the images\n input_dir = r\"/kaggle/input/skin-disease-dataset/dataset/test\"\n # Output folder for renamed images\n output_dir = r\"/kaggle/working/renamed_Test\"\n\n # Ensure the output directory exists\n os.makedirs(output_dir, exist_ok=True)\n\n # Dictionary to track counts for each class\n class_counts = {}\n\n # Traverse through each subdirectory\n for root, dirs, files in os.walk(input_dir):\n     for file_name in files:\n         # Full path of the image\n         img_path = os.path.join(root, file_name)\n\n         # Skip non-image files\n         if not file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n             print(f\"Skipping non-image file: {file_name}\")\n             continue\n\n         # Get the folder name (class name) as the class identifier\n         class_name = os.path.basename(root)\n\n         # Initialize or increment the count for this class\n         if class_name not in class_counts:\n             class_counts[class_name] = 1\n         else:\n             class_counts[class_name] += 1\n\n         # Generate new file name in the format ClassName(Count).Extension\n         count = class_counts[class_name]\n         ext = os.path.splitext(file_name)[1]  # Get file extension\n         new_name = f\"{class_name}({count}){ext}\"\n         new_path = os.path.join(output_dir, new_name)\n\n         # Copy and rename the file to the output directory\n         shutil.copy(img_path, new_path)\n\n # Print the total number of images for each class\n print(\"\\nImage counts by class:\")\n for class_name, count in class_counts.items():\n     print(f\"{class_name}: {count} images\")\n\n print(\"\\nRenaming and consolidation complete! (test)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T17:04:18.535838Z","iopub.execute_input":"2025-02-14T17:04:18.536055Z","iopub.status.idle":"2025-02-14T17:04:38.098963Z","shell.execute_reply.started":"2025-02-14T17:04:18.536036Z","shell.execute_reply":"2025-02-14T17:04:38.098243Z"}},"outputs":[{"name":"stdout","text":"\nImage counts by class:\nEczema: 200 images\nMelanoma: 200 images\nBasal Cell: 200 images\nSeborrheic: 200 images\nAtopic Dermatitis: 200 images\nMelanocytic: 200 images\nWarts Molluscum: 200 images\nPsoriasis: 200 images\nTinea Ringworms Candidiasis: 200 images\n\nRenaming and consolidation complete! (test)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import InceptionV3  # Replace EfficientNetB3 with InceptionV3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization, Dense, Dropout\nfrom tensorflow.keras.models import Model\n\n# Define class mapping (updated with \"Basal Cell\")\nclass_mapping = {\n    \"Seborrheic\": 0,\n    \"Melanocytic\": 1,\n    \"Melanoma\": 2,\n    \"Eczema\": 3,\n    \"Basal_Cell\": 4,  # New class added\n}\n\n# Preprocess images: resize and normalize\ndef preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Warning: {image_path} could not be loaded.\")\n        return None\n\n    resized_image = cv2.resize(image, (299, 299))  # Resizing to 299x299 for InceptionV3\n    img_normalized = resized_image.astype('float32') / 255.0  # Normalize to [0, 1]\n    return img_normalized\n\ndef load_data_from_single_folder(folder):\n    images = []\n    labels = []\n\n    # Get a sorted list of image filenames\n    image_files = sorted([f for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n\n    for image_name in image_files:\n        image_path = os.path.join(folder, image_name)\n\n        # Extract the label from the filename (before the parentheses)\n        label = image_name.split('(')[0].strip().replace(' ', '_')  # Handle spaces and extract class name\n        \n        if label in class_mapping:\n            label_index = class_mapping[label]  # Map label to integer\n        else:\n            #print(f\"Warning: Label {label} not found in mapping. Skipping image.\")\n            continue\n\n        # Preprocess the image\n        preprocessed_image = preprocess_image(image_path)\n        if preprocessed_image is not None:\n            images.append(preprocessed_image)\n            labels.append(label_index)\n\n    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n    return np.array(images), np.array(labels)\n\ndef predict_test_data(model, test_images):\n    # Predict probabilities for each class\n    test_preds = model.predict(test_images)\n    # Get the predicted class indices\n    test_pred_classes = np.argmax(test_preds, axis=1)\n    return test_pred_classes, test_preds\n\n# Map predicted indices to class names\ndef map_classes_to_names(pred_classes):\n    label_to_class = {v: k for k, v in class_mapping.items()}\n    pred_class_names = [label_to_class[pred] for pred in pred_classes]\n    return pred_class_names\n\n# Calculate test accuracy\ndef calculate_accuracy(true_labels, pred_labels):\n    accuracy = np.mean(true_labels == pred_labels)\n    return accuracy\n\n# Paths for train and test folders\ntrain_folder = r'/kaggle/working/renamed_train'\ntest_folder = r\"/kaggle/working/renamed_Test\"\n\n# Load data\nX_train, y_train = load_data_from_single_folder(train_folder)\n# X_test, y_test = load_data_from_single_folder(test_folder)\n\n# === Step 6: Class Distribution Analysis ===\nclass_counts = pd.Series(y_train).value_counts()\nclass_names = {v: k for k, v in class_mapping.items()}  # Reverse the mapping\nclass_counts_named = class_counts.rename(index=class_names)\n\nprint(\"\\nClass counts (class names):\")\nprint(class_counts_named)\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n# === Step 7: Balance Classes to Max Class Size Using Augmentation ===\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nmax_class_size = class_counts.max()  # Maximum size among all classes\naugmented_images = []\naugmented_labels = []\n\n# Create augmented images for each class\nfor label in np.unique(y_train):\n    class_images = X_train[y_train == label]\n    current_class_size = class_counts[label]\n\n    augmented = datagen.flow(class_images, batch_size=1)\n    for _ in range(max_class_size - current_class_size):\n        augmented_images.append(next(augmented)[0])\n        augmented_labels.append(label)\n\n# If augmented images are created, concatenate them with the original data\nif augmented_images:  # Ensure there are augmented images to add\n    X_train = np.concatenate([X_train, np.array(augmented_images)])\n    y_train = np.concatenate([y_train, np.array(augmented_labels)])\n\n# Check new class distribution\nnew_class_counts = pd.Series(y_train).value_counts()\nnew_class_counts_named = new_class_counts.rename(index=class_names)\n\nprint(\"\\nNew class counts after augmentation (class names):\")\nprint(new_class_counts_named)\n\n# Check class distribution for test set\ntrain_class_counts = pd.Series(y_train).value_counts().rename(index=class_names)\ntest_class_counts = pd.Series(y_test).value_counts().rename(index=class_names)\n\nprint(\"\\nClass counts in training set:\")\nprint(train_class_counts)\n\nprint(\"\\nClass counts in test set:\")\nprint(test_class_counts)\n\n# Define a dense block function\ndef dense_block(units, dropout_rate):\n    def block(x):\n        x = Dense(units, activation='relu')(x)\n        x = BatchNormalization()(x)\n        x = Dropout(dropout_rate)(x)\n        return x\n    return block\n\n# Load InceptionV3 model with pretrained weights, excluding the top layers\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3), pooling=None)\n\n# Fine-tune InceptionV3 (Adding some custom layers on top)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = BatchNormalization()(x)\nx = dense_block(128, 0.5)(x)\nx = dense_block(32, 0.2)(x)\npredictions = Dense(len(class_mapping), activation=\"softmax\")(x)  # Output layer with softmax activation\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Unfreeze the last 10 layers of the base model\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\n# Compile the model with Adam optimizer\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n              loss='sparse_categorical_crossentropy',  # Sparse because labels are integers\n              metrics=['accuracy'])\n\n# Learning rate scheduler\ncallbacks = [\n    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n]\n\n# Train the model with test data as validation\nepochs = 50\nbatch_size = 16\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n                    validation_data=(X_test, y_test), callbacks=callbacks, shuffle=False)\n\n# Predict on the test data\ny_test_pred = model.predict(X_test)\ny_test_pred_classes = np.argmax(y_test_pred, axis=1)\n\n# Confusion matrix for test set\ntest_conf_matrix = confusion_matrix(y_test, y_test_pred_classes)\nprint(f\"Test Confusion Matrix:\\n{test_conf_matrix}\")\n\n# Calculate overall accuracy for the test set\ntest_accuracy = np.mean(y_test_pred_classes == y_test)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n# Print the first 10 predictions and actual class names from test set\nlabel_to_class = {v: k for k, v in class_mapping.items()}\n\nfor i in range(10):\n    predicted_class = label_to_class[y_test_pred_classes[i]]\n    actual_class = label_to_class[y_test[i]]\n    print(f\"Predicted: {predicted_class}, Actual: {actual_class}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T17:34:22.772774Z","iopub.execute_input":"2025-02-14T17:34:22.773172Z","iopub.status.idle":"2025-02-14T18:07:43.702613Z","shell.execute_reply.started":"2025-02-14T17:34:22.773137Z","shell.execute_reply":"2025-02-14T18:07:43.701754Z"}},"outputs":[{"name":"stdout","text":"Loaded 4999 images and 4999 labels.\n\nClass counts (class names):\nBasal_Cell     1000\nMelanocytic    1000\nMelanoma       1000\nSeborrheic     1000\nEczema          999\nName: count, dtype: int64\n\nNew class counts after augmentation (class names):\nMelanocytic    800\nBasal_Cell     800\nEczema         800\nSeborrheic     800\nMelanoma       800\nName: count, dtype: int64\n\nClass counts in training set:\nMelanocytic    800\nBasal_Cell     800\nEczema         800\nSeborrheic     800\nMelanoma       800\nName: count, dtype: int64\n\nClass counts in test set:\nMelanocytic    200\nEczema         200\nSeborrheic     200\nMelanoma       200\nBasal_Cell     200\nName: count, dtype: int64\nEpoch 1/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 205ms/step - accuracy: 0.3228 - loss: 1.9080 - val_accuracy: 0.6640 - val_loss: 0.9637 - learning_rate: 1.0000e-05\nEpoch 2/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.6641 - loss: 0.9517 - val_accuracy: 0.7730 - val_loss: 0.6712 - learning_rate: 1.0000e-05\nEpoch 3/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 145ms/step - accuracy: 0.7620 - loss: 0.6763 - val_accuracy: 0.8130 - val_loss: 0.5679 - learning_rate: 1.0000e-05\nEpoch 4/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.8482 - loss: 0.4946 - val_accuracy: 0.8260 - val_loss: 0.5228 - learning_rate: 1.0000e-05\nEpoch 5/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 146ms/step - accuracy: 0.8808 - loss: 0.4271 - val_accuracy: 0.8440 - val_loss: 0.4765 - learning_rate: 1.0000e-05\nEpoch 6/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 146ms/step - accuracy: 0.9096 - loss: 0.3506 - val_accuracy: 0.8580 - val_loss: 0.4514 - learning_rate: 1.0000e-05\nEpoch 7/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 146ms/step - accuracy: 0.9267 - loss: 0.3034 - val_accuracy: 0.8750 - val_loss: 0.4301 - learning_rate: 1.0000e-05\nEpoch 8/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9464 - loss: 0.2711 - val_accuracy: 0.8790 - val_loss: 0.4125 - learning_rate: 1.0000e-05\nEpoch 9/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9598 - loss: 0.2360 - val_accuracy: 0.8790 - val_loss: 0.4013 - learning_rate: 1.0000e-05\nEpoch 10/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9561 - loss: 0.2189 - val_accuracy: 0.8810 - val_loss: 0.3813 - learning_rate: 1.0000e-05\nEpoch 11/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9766 - loss: 0.1912 - val_accuracy: 0.8850 - val_loss: 0.3700 - learning_rate: 1.0000e-05\nEpoch 12/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9800 - loss: 0.1703 - val_accuracy: 0.8890 - val_loss: 0.3648 - learning_rate: 1.0000e-05\nEpoch 13/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9810 - loss: 0.1563 - val_accuracy: 0.8880 - val_loss: 0.3507 - learning_rate: 1.0000e-05\nEpoch 14/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9848 - loss: 0.1400 - val_accuracy: 0.8900 - val_loss: 0.3433 - learning_rate: 1.0000e-05\nEpoch 15/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9876 - loss: 0.1340 - val_accuracy: 0.8940 - val_loss: 0.3343 - learning_rate: 1.0000e-05\nEpoch 16/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9847 - loss: 0.1305 - val_accuracy: 0.8980 - val_loss: 0.3244 - learning_rate: 1.0000e-05\nEpoch 17/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9886 - loss: 0.1197 - val_accuracy: 0.9000 - val_loss: 0.3195 - learning_rate: 1.0000e-05\nEpoch 18/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9898 - loss: 0.1138 - val_accuracy: 0.9010 - val_loss: 0.3241 - learning_rate: 1.0000e-05\nEpoch 19/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9958 - loss: 0.0985 - val_accuracy: 0.8950 - val_loss: 0.3097 - learning_rate: 1.0000e-05\nEpoch 20/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9932 - loss: 0.0988 - val_accuracy: 0.8980 - val_loss: 0.3101 - learning_rate: 1.0000e-05\nEpoch 21/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 144ms/step - accuracy: 0.9938 - loss: 0.0917 - val_accuracy: 0.9070 - val_loss: 0.2983 - learning_rate: 1.0000e-05\nEpoch 22/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 146ms/step - accuracy: 0.9943 - loss: 0.0846 - val_accuracy: 0.9000 - val_loss: 0.2997 - learning_rate: 1.0000e-05\nEpoch 23/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 146ms/step - accuracy: 0.9927 - loss: 0.0821 - val_accuracy: 0.9080 - val_loss: 0.2987 - learning_rate: 1.0000e-05\nEpoch 24/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9958 - loss: 0.0789 - val_accuracy: 0.9100 - val_loss: 0.2985 - learning_rate: 1.0000e-05\nEpoch 25/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 145ms/step - accuracy: 0.9952 - loss: 0.0756 - val_accuracy: 0.9100 - val_loss: 0.2927 - learning_rate: 1.0000e-05\nEpoch 26/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9941 - loss: 0.0706 - val_accuracy: 0.9180 - val_loss: 0.2890 - learning_rate: 1.0000e-05\nEpoch 27/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9941 - loss: 0.0715 - val_accuracy: 0.9140 - val_loss: 0.2912 - learning_rate: 1.0000e-05\nEpoch 28/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9974 - loss: 0.0654 - val_accuracy: 0.9140 - val_loss: 0.2866 - learning_rate: 1.0000e-05\nEpoch 29/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9944 - loss: 0.0616 - val_accuracy: 0.9140 - val_loss: 0.2809 - learning_rate: 1.0000e-05\nEpoch 30/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9954 - loss: 0.0607 - val_accuracy: 0.9140 - val_loss: 0.2791 - learning_rate: 1.0000e-05\nEpoch 31/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9968 - loss: 0.0560 - val_accuracy: 0.9220 - val_loss: 0.2717 - learning_rate: 1.0000e-05\nEpoch 32/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9965 - loss: 0.0567 - val_accuracy: 0.9190 - val_loss: 0.2678 - learning_rate: 1.0000e-05\nEpoch 33/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9973 - loss: 0.0518 - val_accuracy: 0.9190 - val_loss: 0.2630 - learning_rate: 1.0000e-05\nEpoch 34/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9973 - loss: 0.0519 - val_accuracy: 0.9260 - val_loss: 0.2600 - learning_rate: 1.0000e-05\nEpoch 35/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9971 - loss: 0.0465 - val_accuracy: 0.9250 - val_loss: 0.2562 - learning_rate: 1.0000e-05\nEpoch 36/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9993 - loss: 0.0444 - val_accuracy: 0.9260 - val_loss: 0.2588 - learning_rate: 1.0000e-05\nEpoch 37/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9991 - loss: 0.0446 - val_accuracy: 0.9310 - val_loss: 0.2559 - learning_rate: 1.0000e-05\nEpoch 38/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9992 - loss: 0.0417 - val_accuracy: 0.9330 - val_loss: 0.2472 - learning_rate: 1.0000e-05\nEpoch 39/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 145ms/step - accuracy: 0.9963 - loss: 0.0448 - val_accuracy: 0.9230 - val_loss: 0.2815 - learning_rate: 1.0000e-05\nEpoch 40/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 146ms/step - accuracy: 0.9963 - loss: 0.0418 - val_accuracy: 0.9260 - val_loss: 0.2636 - learning_rate: 1.0000e-05\nEpoch 41/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9982 - loss: 0.0387 - val_accuracy: 0.9280 - val_loss: 0.2679 - learning_rate: 1.0000e-05\nEpoch 42/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 146ms/step - accuracy: 0.9981 - loss: 0.0364 - val_accuracy: 0.9210 - val_loss: 0.2761 - learning_rate: 1.0000e-05\nEpoch 43/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9967 - loss: 0.0383\nEpoch 43: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 144ms/step - accuracy: 0.9967 - loss: 0.0383 - val_accuracy: 0.9310 - val_loss: 0.2513 - learning_rate: 1.0000e-05\nEpoch 44/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 148ms/step - accuracy: 0.9974 - loss: 0.0363 - val_accuracy: 0.9350 - val_loss: 0.2429 - learning_rate: 5.0000e-06\nEpoch 45/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9994 - loss: 0.0316 - val_accuracy: 0.9320 - val_loss: 0.2559 - learning_rate: 5.0000e-06\nEpoch 46/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9991 - loss: 0.0320 - val_accuracy: 0.9310 - val_loss: 0.2508 - learning_rate: 5.0000e-06\nEpoch 47/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9993 - loss: 0.0298 - val_accuracy: 0.9320 - val_loss: 0.2532 - learning_rate: 5.0000e-06\nEpoch 48/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9999 - loss: 0.0287 - val_accuracy: 0.9310 - val_loss: 0.2511 - learning_rate: 5.0000e-06\nEpoch 49/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9999 - loss: 0.0272\nEpoch 49: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9999 - loss: 0.0272 - val_accuracy: 0.9340 - val_loss: 0.2498 - learning_rate: 5.0000e-06\nEpoch 50/50\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.9990 - loss: 0.0284 - val_accuracy: 0.9330 - val_loss: 0.2512 - learning_rate: 2.5000e-06\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 267ms/step\nTest Confusion Matrix:\n[[179   0   0  21   0]\n [  0 183   9   1   7]\n [  0   4 194   0   2]\n [ 15   0   0 185   0]\n [  0   3   3   0 194]]\nTest Accuracy: 93.50%\nPredicted: Melanocytic, Actual: Melanocytic\nPredicted: Eczema, Actual: Eczema\nPredicted: Seborrheic, Actual: Seborrheic\nPredicted: Melanoma, Actual: Melanoma\nPredicted: Melanoma, Actual: Melanoma\nPredicted: Seborrheic, Actual: Seborrheic\nPredicted: Seborrheic, Actual: Seborrheic\nPredicted: Melanocytic, Actual: Melanocytic\nPredicted: Seborrheic, Actual: Seborrheic\nPredicted: Melanoma, Actual: Melanoma\n","output_type":"stream"}],"execution_count":6}]}