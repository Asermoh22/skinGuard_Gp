{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8479214,"sourceType":"datasetVersion","datasetId":5057198}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\n# Input folder containing the images\ninput_dir = r\"/kaggle/input/skin-disease-dataset/dataset/train\"\n# Output folder for renamed images\noutput_dir = r\"/kaggle/working/renamed_train\"\n\n# Ensure the output directory exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Dictionary to track counts for each class\nclass_counts = {}\n\n# Traverse through each subdirectory\nfor root, dirs, files in os.walk(input_dir):\n for file_name in files:\n     # Full path of the image\n     img_path = os.path.join(root, file_name)\n\n     # Skip non-image files\n     if not file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n         print(f\"Skipping non-image file: {file_name}\")\n         continue\n\n     # Get the folder name (class name) as the class identifier\n     class_name = os.path.basename(root)\n\n     # Initialize or increment the count for this class\n     if class_name not in class_counts:\n         class_counts[class_name] = 1\n     else:\n         class_counts[class_name] += 1\n\n     # Generate new file name in the format ClassName(Count).Extension\n     count = class_counts[class_name]\n     ext = os.path.splitext(file_name)[1]  # Get file extension\n     new_name = f\"{class_name}({count}){ext}\"\n     new_path = os.path.join(output_dir, new_name)\n\n     # Copy and rename the file to the output directory\n     shutil.copy(img_path, new_path)\n\n# Print the total number of images for each class\nprint(\"\\nImage counts by class:\")\nfor class_name, count in class_counts.items():\n print(f\"{class_name}: {count} images\")\n\nprint(\"\\nRenaming and consolidation complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:00:06.537124Z","iopub.execute_input":"2025-05-01T11:00:06.537885Z","iopub.status.idle":"2025-05-01T11:01:10.571674Z","shell.execute_reply.started":"2025-05-01T11:00:06.537850Z","shell.execute_reply":"2025-05-01T11:01:10.570892Z"}},"outputs":[{"name":"stdout","text":"\nImage counts by class:\nEczema: 999 images\nMelanoma: 1000 images\nBasal Cell: 1000 images\nSeborrheic: 1000 images\nAtopic Dermatitis: 1000 images\nMelanocytic: 1000 images\nBenign Keratosis: 1201 images\nWarts Molluscum: 1000 images\nPsoriasis: 1000 images\nTinea Ringworms Candidiasis: 990 images\n\nRenaming and consolidation complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\n# --- Class mapping ---\nclass_mapping = {\n    \"Seborrheic\": 0,\n    \"Melanocytic\": 1,\n    \"Melanoma\": 2,\n    \"Eczema\": 3,\n    \"Basal_Cell\": 4,\n    \"Atopic_Dermatitis\": 5,\n    \"Benign_Keratosis\": 6,\n    \"Warts_Molluscum\": 7,\n    \"Psoriasis\": 8,\n    \"Tinea_Ringworms_Candidiasis\": 9\n}\nlabel_to_class = {v: k for k, v in class_mapping.items()}\n\n# --- Preprocess image ---\ndef preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Warning: {image_path} could not be loaded.\")\n        return None\n    resized_image = cv2.resize(image, (224, 224))\n    img_normalized = resized_image.astype('float32') / 255.0\n    return img_normalized\n\n# --- Load data from folder ---\ndef load_data_from_single_folder(folder, max_per_class=1000):\n    images = []\n    labels = []\n    class_counts = {i: 0 for i in range(len(class_mapping))}\n\n    image_files = sorted([f for f in os.listdir(folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n\n    for image_name in image_files:\n        image_path = os.path.join(folder, image_name)\n        label = image_name.split('(')[0].strip().replace(' ', '_')\n\n        if label not in class_mapping:\n            continue\n        label_index = class_mapping[label]\n\n        if class_counts[label_index] >= max_per_class:\n            continue\n\n        preprocessed_image = preprocess_image(image_path)\n        if preprocessed_image is not None:\n            images.append(preprocessed_image)\n            labels.append(label_index)\n            class_counts[label_index] += 1\n\n        if all(count >= max_per_class for count in class_counts.values()):\n            break\n\n    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n    return np.array(images), np.array(labels)\n\n# --- Load and prepare data ---\ntrain_folder = r'/kaggle/working/renamed_train'\nX, y = load_data_from_single_folder(train_folder, max_per_class=1000)\n\n# Shuffle\nindices = np.arange(X.shape[0])\nnp.random.shuffle(indices)\nX = X[indices]\ny = y[indices]\n\n# One-hot encode\ny = to_categorical(y, num_classes=10)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=np.argmax(y, axis=1)\n)\n\n# --- Data Augmentation ---\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\ntest_datagen = ImageDataGenerator()\n\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=32)\ntest_generator = test_datagen.flow(X_test, y_test, batch_size=32)\n\n\n\n# --- Build Model ---\n# base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n# base_model.trainable = False  # Freeze base\n\n# model = models.Sequential([\n#     base_model,\n#     layers.GlobalAveragePooling2D(),\n#     layers.Dense(256, activation='relu'),\n#     layers.Dropout(0.5),\n#     layers.Dense(10, activation='softmax')\n# ])\n\n# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# # --- Callbacks ---\n# early_stop = EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n# lr_reduce = ReduceLROnPlateau(patience=3, factor=0.2, monitor='val_loss')\n\n# # --- Train ---\n# model.fit(\n#     train_generator,\n#     validation_data=test_generator,\n#     epochs=30,\n#     callbacks=[early_stop, lr_reduce]\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:02:38.530318Z","iopub.execute_input":"2025-05-01T11:02:38.530842Z","iopub.status.idle":"2025-05-01T11:03:39.792306Z","shell.execute_reply.started":"2025-05-01T11:02:38.530816Z","shell.execute_reply":"2025-05-01T11:03:39.791703Z"}},"outputs":[{"name":"stderr","text":"2025-05-01 11:02:40.763390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746097360.965471      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746097361.026966      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loaded 9989 images and 9989 labels.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:04:44.906777Z","iopub.execute_input":"2025-05-01T11:04:44.907360Z","iopub.status.idle":"2025-05-01T11:04:44.911742Z","shell.execute_reply.started":"2025-05-01T11:04:44.907333Z","shell.execute_reply":"2025-05-01T11:04:44.911128Z"}},"outputs":[{"name":"stdout","text":"250\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from tensorflow.keras.applications import DenseNet121\nbase_model = DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n\nmodel = tf.keras.Sequential([\n    base_model, \n    layers.GlobalAveragePooling2D(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')  # Output layer (5 classes)\n])\n\n# Unfreeze last few layers of base model for fine-tuning\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\n# Compile model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nprint(model.summary())\n\n# Callbacks for early stopping and learning rate reduction\ncallbacks = [\n    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n]\n\n# Train the model\nepochs = 50\nbatch_size = 16\n# history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n#                     validation_data=(X_test, y_test), callbacks=callbacks, shuffle=True)\n\n# # --- Train ---\nmodel.fit(\n    train_generator,\n    validation_data=test_generator,\n    epochs=100,\n    callbacks=callbacks, \n    batch_size=batch_size,\n    shuffle=True\n)\n# Predict on the test set\ny_test_pred = model.predict(X_test)\ny_test_pred_classes = np.argmax(y_test_pred, axis=1)\ny_test_actual_classes = np.argmax(y_test, axis=1)\n\n# Confusion matrix\ntest_conf_matrix = confusion_matrix(y_test_actual_classes, y_test_pred_classes)\nprint(f\"Test Confusion Matrix:\\n{test_conf_matrix}\")\n\n# Calculate accuracy\ntest_accuracy = np.mean(y_test_pred_classes == y_test_actual_classes)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n# Print first 10 predictions\nfor i in range(10):\n    predicted_class = label_to_class[y_test_pred_classes[i]]\n    actual_class = label_to_class[y_test_actual_classes[i]]\n    print(f\"Predicted: {predicted_class}, Actual: {actual_class}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:06:05.143721Z","iopub.execute_input":"2025-05-01T11:06:05.144011Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ densenet121 (\u001b[38;5;33mFunctional\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │       \u001b[38;5;34m7,037,504\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m65,600\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ densenet121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,103,754\u001b[0m (27.10 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,103,754</span> (27.10 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,020,106\u001b[0m (26.78 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,020,106</span> (26.78 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m83,648\u001b[0m (326.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83,648</span> (326.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746097658.085646      93 service.cc:148] XLA service 0x7c9658001ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746097658.086317      93 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1746097666.772259      93 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1746097744.993506      93 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 715ms/step - accuracy: 0.4570 - loss: 1.5029 - val_accuracy: 0.6902 - val_loss: 0.8558 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 299ms/step - accuracy: 0.7149 - loss: 0.7761 - val_accuracy: 0.7122 - val_loss: 0.8100 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.7907 - loss: 0.5956 - val_accuracy: 0.5806 - val_loss: 1.2900 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 297ms/step - accuracy: 0.8337 - loss: 0.4591 - val_accuracy: 0.6547 - val_loss: 1.0127 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 300ms/step - accuracy: 0.8616 - loss: 0.3877 - val_accuracy: 0.7477 - val_loss: 0.7385 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.8909 - loss: 0.3107 - val_accuracy: 0.7192 - val_loss: 0.8624 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 302ms/step - accuracy: 0.9352 - loss: 0.1951 - val_accuracy: 0.7508 - val_loss: 0.8752 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.9529 - loss: 0.1508\nEpoch 10: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.9529 - loss: 0.1508 - val_accuracy: 0.7628 - val_loss: 0.7930 - learning_rate: 1.0000e-04\nEpoch 11/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.9663 - loss: 0.1126 - val_accuracy: 0.8258 - val_loss: 0.6065 - learning_rate: 5.0000e-05\nEpoch 12/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 299ms/step - accuracy: 0.9826 - loss: 0.0677 - val_accuracy: 0.8043 - val_loss: 0.7150 - learning_rate: 5.0000e-05\nEpoch 13/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.9776 - loss: 0.0732 - val_accuracy: 0.8123 - val_loss: 0.6951 - learning_rate: 5.0000e-05\nEpoch 14/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 302ms/step - accuracy: 0.9809 - loss: 0.0624 - val_accuracy: 0.8143 - val_loss: 0.6999 - learning_rate: 5.0000e-05\nEpoch 15/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.9870 - loss: 0.0534 - val_accuracy: 0.8038 - val_loss: 0.7514 - learning_rate: 5.0000e-05\nEpoch 16/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.9824 - loss: 0.0620\nEpoch 16: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 303ms/step - accuracy: 0.9824 - loss: 0.0620 - val_accuracy: 0.8093 - val_loss: 0.7183 - learning_rate: 5.0000e-05\nEpoch 17/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 299ms/step - accuracy: 0.9876 - loss: 0.0426 - val_accuracy: 0.8348 - val_loss: 0.7065 - learning_rate: 2.5000e-05\nEpoch 18/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.9907 - loss: 0.0336 - val_accuracy: 0.8273 - val_loss: 0.7106 - learning_rate: 2.5000e-05\nEpoch 19/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 297ms/step - accuracy: 0.9931 - loss: 0.0259 - val_accuracy: 0.8253 - val_loss: 0.7713 - learning_rate: 2.5000e-05\nEpoch 20/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 299ms/step - accuracy: 0.9937 - loss: 0.0259 - val_accuracy: 0.8378 - val_loss: 0.7163 - learning_rate: 2.5000e-05\nEpoch 21/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.9934 - loss: 0.0238\nEpoch 21: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 297ms/step - accuracy: 0.9934 - loss: 0.0238 - val_accuracy: 0.8358 - val_loss: 0.7342 - learning_rate: 2.5000e-05\nEpoch 22/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 297ms/step - accuracy: 0.9927 - loss: 0.0265 - val_accuracy: 0.8333 - val_loss: 0.7243 - learning_rate: 1.2500e-05\nEpoch 23/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 300ms/step - accuracy: 0.9920 - loss: 0.0238 - val_accuracy: 0.8408 - val_loss: 0.7233 - learning_rate: 1.2500e-05\nEpoch 24/100\n\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 298ms/step - accuracy: 0.9940 - loss: 0.0223 - val_accuracy: 0.8288 - val_loss: 0.7624 - learning_rate: 1.2500e-05\nEpoch 25/100\n\u001b[1m227/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 290ms/step - accuracy: 0.9942 - loss: 0.0213","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.applications import InceptionResNetV2\n# from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# # Define image size and input shape (299x299 for InceptionResNetV2)\n\n\n# # Load the pre-trained InceptionResNetV2 model without the top layer\n# base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299,299,3))\n\n# # Freeze the base model (pre-trained weights will not change during training)\n# base_model.trainable = False\n\n# model = tf.keras.Sequential([\n#     base_model, \n#     tf.keras.layers.GlobalAveragePooling2D(),  # Pooling layer\n#     tf.keras.layers.Dense(64, activation='relu'),  # Fully connected layer\n#     tf.keras.layers.Dense(5, activation='softmax')  # Output layer (5 classes)\n# ])\n# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n#               loss='categorical_crossentropy',\n#               metrics=['accuracy'])\n# print(model.summary())\n\n# for layer in base_model.layers[-4:]:  # Unfreeze the last 4 layers\n#     layer.trainable = True\n\n# # Compile the model with Adam optimizer\n\n\n\n\n\n\n\nfrom tensorflow.keras.applications import DenseNet121\nbase_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n\nmodel = tf.keras.Sequential([\n    base_model, \n    layers.GlobalAveragePooling2D(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')  # Output layer (5 classes)\n])\n\n# Unfreeze last few layers of base model for fine-tuning\nfor layer in base_model.layers[-20:]:\n    layer.trainable = True\n\n# Compile model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nprint(model.summary())\n\n# Callbacks for early stopping and learning rate reduction\ncallbacks = [\n    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n]\n\n# Train the model\nepochs = 50\nbatch_size = 16\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n                    validation_data=(X_test, y_test), callbacks=callbacks, shuffle=True)\n\n# Predict on the test set\ny_test_pred = model.predict(X_test)\ny_test_pred_classes = np.argmax(y_test_pred, axis=1)\ny_test_actual_classes = np.argmax(y_test, axis=1)\n\n# Confusion matrix\ntest_conf_matrix = confusion_matrix(y_test_actual_classes, y_test_pred_classes)\nprint(f\"Test Confusion Matrix:\\n{test_conf_matrix}\")\n\n# Calculate accuracy\ntest_accuracy = np.mean(y_test_pred_classes == y_test_actual_classes)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n# Print first 10 predictions\nfor i in range(10):\n    predicted_class = label_to_class[y_test_pred_classes[i]]\n    actual_class = label_to_class[y_test_actual_classes[i]]\n    print(f\"Predicted: {predicted_class}, Actual: {actual_class}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T22:18:19.631809Z","iopub.execute_input":"2025-04-26T22:18:19.632110Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ inception_resnet_v2 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1536\u001b[0m)          │      \u001b[38;5;34m54,336,736\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m98,368\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ inception_resnet_v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">54,336,736</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,368</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,435,754\u001b[0m (207.66 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,435,754</span> (207.66 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m54,375,210\u001b[0m (207.42 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,375,210</span> (207.42 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m60,544\u001b[0m (236.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">60,544</span> (236.50 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\nEpoch 1/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 352ms/step - accuracy: 0.4860 - loss: 1.3755 - val_accuracy: 0.7082 - val_loss: 1.9947 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 207ms/step - accuracy: 0.7830 - loss: 0.6040 - val_accuracy: 0.7377 - val_loss: 0.8314 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.8902 - loss: 0.3353 - val_accuracy: 0.7377 - val_loss: 0.8209 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 207ms/step - accuracy: 0.9323 - loss: 0.2025 - val_accuracy: 0.7528 - val_loss: 0.9259 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9502 - loss: 0.1570 - val_accuracy: 0.7252 - val_loss: 1.1081 - learning_rate: 1.0000e-04\nEpoch 6/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 207ms/step - accuracy: 0.9642 - loss: 0.1077 - val_accuracy: 0.7548 - val_loss: 1.2363 - learning_rate: 1.0000e-04\nEpoch 7/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 207ms/step - accuracy: 0.9668 - loss: 0.1025 - val_accuracy: 0.7603 - val_loss: 1.8950 - learning_rate: 1.0000e-04\nEpoch 8/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9659 - loss: 0.0914\nEpoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9658 - loss: 0.0915 - val_accuracy: 0.7573 - val_loss: 1.1983 - learning_rate: 1.0000e-04\nEpoch 9/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9771 - loss: 0.0581 - val_accuracy: 0.8038 - val_loss: 0.8726 - learning_rate: 5.0000e-05\nEpoch 10/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9895 - loss: 0.0315 - val_accuracy: 0.7918 - val_loss: 1.0431 - learning_rate: 5.0000e-05\nEpoch 11/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9897 - loss: 0.0268 - val_accuracy: 0.7928 - val_loss: 1.0420 - learning_rate: 5.0000e-05\nEpoch 12/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9910 - loss: 0.0209 - val_accuracy: 0.7803 - val_loss: 1.2335 - learning_rate: 5.0000e-05\nEpoch 13/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9874 - loss: 0.0262\nEpoch 13: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9874 - loss: 0.0262 - val_accuracy: 0.7913 - val_loss: 1.2017 - learning_rate: 5.0000e-05\nEpoch 14/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 207ms/step - accuracy: 0.9916 - loss: 0.0205 - val_accuracy: 0.8053 - val_loss: 1.0159 - learning_rate: 2.5000e-05\nEpoch 15/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9928 - loss: 0.0134 - val_accuracy: 0.8053 - val_loss: 1.0265 - learning_rate: 2.5000e-05\nEpoch 16/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9924 - loss: 0.0134 - val_accuracy: 0.7953 - val_loss: 1.1878 - learning_rate: 2.5000e-05\nEpoch 17/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9925 - loss: 0.0139 - val_accuracy: 0.7913 - val_loss: 1.2423 - learning_rate: 2.5000e-05\nEpoch 18/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9929 - loss: 0.0150\nEpoch 18: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.9929 - loss: 0.0150 - val_accuracy: 0.7993 - val_loss: 1.2153 - learning_rate: 2.5000e-05\nEpoch 19/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 207ms/step - accuracy: 0.9937 - loss: 0.0120 - val_accuracy: 0.8073 - val_loss: 1.1983 - learning_rate: 1.2500e-05\nEpoch 20/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 207ms/step - accuracy: 0.9929 - loss: 0.0113 - val_accuracy: 0.8028 - val_loss: 1.2315 - learning_rate: 1.2500e-05\nEpoch 21/50\n\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.9936 - loss: 0.0108","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}