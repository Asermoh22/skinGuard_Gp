{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8479214,"sourceType":"datasetVersion","datasetId":5057198}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" import os\n import shutil\n\n # Input folder containing the images\n input_dir = r\"/kaggle/input/skin-disease-dataset/dataset/train\"\n # Output folder for renamed images\n output_dir = r\"/kaggle/working/renamed_train\"\n\n # Ensure the output directory exists\n os.makedirs(output_dir, exist_ok=True)\n\n # Dictionary to track counts for each class\n class_counts = {}\n\n # Traverse through each subdirectory\n for root, dirs, files in os.walk(input_dir):\n     for file_name in files:\n         # Full path of the image\n         img_path = os.path.join(root, file_name)\n\n         # Skip non-image files\n         if not file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n             print(f\"Skipping non-image file: {file_name}\")\n             continue\n\n         # Get the folder name (class name) as the class identifier\n         class_name = os.path.basename(root)\n\n         # Initialize or increment the count for this class\n         if class_name not in class_counts:\n             class_counts[class_name] = 1\n         else:\n             class_counts[class_name] += 1\n\n         # Generate new file name in the format ClassName(Count).Extension\n         count = class_counts[class_name]\n         ext = os.path.splitext(file_name)[1]  # Get file extension\n         new_name = f\"{class_name}({count}){ext}\"\n         new_path = os.path.join(output_dir, new_name)\n\n         # Copy and rename the file to the output directory\n         shutil.copy(img_path, new_path)\n\n # Print the total number of images for each class\n print(\"\\nImage counts by class:\")\n for class_name, count in class_counts.items():\n     print(f\"{class_name}: {count} images\")\n\n print(\"\\nRenaming and consolidation complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-21T20:52:02.107530Z","iopub.execute_input":"2025-04-21T20:52:02.107759Z","iopub.status.idle":"2025-04-21T20:52:16.296481Z","shell.execute_reply.started":"2025-04-21T20:52:02.107734Z","shell.execute_reply":"2025-04-21T20:52:16.295802Z"}},"outputs":[{"name":"stdout","text":"\nImage counts by class:\nEczema: 999 images\nMelanoma: 1000 images\nBasal Cell: 1000 images\nSeborrheic: 1000 images\nAtopic Dermatitis: 1000 images\nMelanocytic: 1000 images\nBenign Keratosis: 1201 images\nWarts Molluscum: 1000 images\nPsoriasis: 1000 images\nTinea Ringworms Candidiasis: 990 images\n\nRenaming and consolidation complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.models import vgg19\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport math\nfrom collections import Counter\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Class mapping and parameters\nclass_mapping = {\n    \"Seborrheic\": 0, \"Melanocytic\": 1, \"Melanoma\": 2, \"Eczema\": 3,\n    \"Basal_Cell\": 4, \"Psoriasis\": 5, \"Tinea_Ringworms_Candidiasis\": 6,\n    \"Warts_Molluscum\": 7, \"Atopic_Dermatitis\": 8\n}\nTARGET_SAMPLES_PER_CLASS = 1500\n\n# DATA BALANCING \ndef load_and_balance_data(folder):\n    \"\"\"Load images and ensure exactly 1500 per class using augmentation\"\"\"\n    class_counts = {cls: 0 for cls in class_mapping.values()}\n    images = []\n    labels = []\n    \n    # First pass: collect natural images\n    raw_data = {cls: [] for cls in class_mapping.values()}\n    for img_name in os.listdir(folder):\n        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n            label_name = img_name.split('(')[0].strip().replace(' ', '_')\n            if label_name in class_mapping:\n                label = class_mapping[label_name]\n                img_path = os.path.join(folder, img_name)\n                img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n                if img is not None:\n                    raw_data[label].append(img)\n    \n    # Second pass: balance classes\n    augmenter = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.3),\n        A.Rotate(limit=25),\n        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    ])\n    \n    for cls, cls_images in raw_data.items():\n        # Add original images\n        for img in cls_images[:min(len(cls_images), TARGET_SAMPLES_PER_CLASS)]:\n            images.append(img)\n            labels.append(cls)\n            class_counts[cls] += 1\n        \n        # Augment to reach target count\n        while class_counts[cls] < TARGET_SAMPLES_PER_CLASS:\n            for img in cls_images:\n                if class_counts[cls] >= TARGET_SAMPLES_PER_CLASS:\n                    break\n                augmented = augmenter(image=img)['image']\n                images.append(augmented)\n                labels.append(cls)\n                class_counts[cls] += 1\n    \n    # Verify balancing\n    print(\"\\nFinal class distribution:\")\n    for cls, count in Counter(labels).items():\n        print(f\"Class {cls}: {count} images\")\n    \n    return images, np.array(labels)\n\n# ENHANCED VGG19\nclass BalancedVGG19(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.base = vgg19(pretrained=True)\n        \n        # Freeze early layers\n        for param in self.base.features[:24].parameters():\n            param.requires_grad = False\n            \n        # Enhanced classifier\n        self.base.classifier = nn.Sequential(\n            nn.Linear(25088, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(1024, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.base(x)\n\n# AUGMENTATIONS\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.RandomCrop(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.3),\n    A.Rotate(limit=25),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    A.CoarseDropout(max_holes=6, max_height=32, max_width=32, p=0.3),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# TRAINING LOOP \ndef train_model():\n    # Load and balance data\n    images, labels = load_and_balance_data(\"/kaggle/working/renamed_train\")\n    X_train, X_val, y_train, y_val = train_test_split(\n        images, labels, test_size=0.2, stratify=labels, random_state=42\n    )\n    \n    # Create datasets\n    class SkinDataset(Dataset):\n        def __init__(self, images, labels, transform=None):\n            self.images = images\n            self.labels = labels\n            self.transform = transform\n        \n        def __len__(self):\n            return len(self.images)\n        \n        def __getitem__(self, idx):\n            img = self.images[idx]\n            label = self.labels[idx]\n            if self.transform:\n                img = self.transform(image=img)['image']\n            return img, label\n    \n    train_dataset = SkinDataset(X_train, y_train, train_transform)\n    val_dataset = SkinDataset(X_val, y_val, val_transform)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n    \n    # Initialize model\n    model = BalancedVGG19(len(class_mapping)).to(device)\n    \n    # Weighted loss (adjust weights based on your dataset)\n    class_weights = torch.tensor([1.0, 1.2, 1.5, 1.0, 1.3, 1.0, 1.1, 1.0, 1.2]).to(device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    \n    # Optimizer and scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)\n    \n    # Training\n    best_val_acc = 0.0\n    for epoch in range(30):\n        model.train()\n        train_loss, correct, total = 0, 0, 0\n        \n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/30\"):\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_acc = 100 * correct / total\n        val_loss, val_acc = validate(model, val_loader, criterion)\n        \n        # Apply accuracy ceiling\n        val_acc_ceil = math.ceil(val_acc * 100) / 100\n        \n        # Update scheduler\n        scheduler.step(val_acc)\n        \n        print(f\"Train Loss: {train_loss/len(train_loader):.4f} | Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            #torch.save(model.state_dict(), \"best_vgg19_model.pth\")\n            #print(f\"New best model saved (Val Acc: {math.ceil(best_val_acc*100)/100:.2f}%)\")\n\ndef validate(model, val_loader, criterion):\n    model.eval()\n    val_loss, correct, total = 0, 0, 0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return val_loss/len(val_loader), 100*correct/total\n\nif __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T21:50:43.865811Z","iopub.execute_input":"2025-04-21T21:50:43.866603Z","iopub.status.idle":"2025-04-21T22:22:27.483440Z","shell.execute_reply.started":"2025-04-21T21:50:43.866574Z","shell.execute_reply":"2025-04-21T22:22:27.482396Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n/tmp/ipykernel_209/616733837.py:109: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=6, max_height=32, max_width=32, p=0.3),\n","output_type":"stream"},{"name":"stdout","text":"\nFinal class distribution:\nClass 0: 1500 images\nClass 1: 1500 images\nClass 2: 1500 images\nClass 3: 1500 images\nClass 4: 1500 images\nClass 5: 1500 images\nClass 6: 1500 images\nClass 7: 1500 images\nClass 8: 1500 images\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nEpoch 1/30: 100%|██████████| 338/338 [00:51<00:00,  6.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.1440 | Acc: 53.37%\nVal Loss: 0.8415 | Acc: 65.48%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 338/338 [00:50<00:00,  6.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8606 | Acc: 65.70%\nVal Loss: 0.8040 | Acc: 68.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7527 | Acc: 69.68%\nVal Loss: 0.7949 | Acc: 69.74%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 338/338 [00:50<00:00,  6.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6610 | Acc: 74.02%\nVal Loss: 0.7690 | Acc: 71.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6246 | Acc: 75.31%\nVal Loss: 0.6914 | Acc: 74.15%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 338/338 [00:50<00:00,  6.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5566 | Acc: 77.99%\nVal Loss: 0.6517 | Acc: 75.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 338/338 [00:50<00:00,  6.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5197 | Acc: 79.68%\nVal Loss: 0.6548 | Acc: 75.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4835 | Acc: 81.66%\nVal Loss: 0.5662 | Acc: 78.70%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 338/338 [00:50<00:00,  6.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4329 | Acc: 83.30%\nVal Loss: 0.6308 | Acc: 78.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 338/338 [00:50<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4040 | Acc: 84.31%\nVal Loss: 0.6215 | Acc: 79.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3866 | Acc: 85.41%\nVal Loss: 0.5603 | Acc: 81.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3589 | Acc: 86.29%\nVal Loss: 0.5525 | Acc: 81.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 338/338 [00:50<00:00,  6.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3338 | Acc: 87.38%\nVal Loss: 0.5348 | Acc: 81.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 338/338 [00:50<00:00,  6.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3163 | Acc: 88.05%\nVal Loss: 0.5232 | Acc: 82.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 338/338 [00:50<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2817 | Acc: 89.72%\nVal Loss: 0.5380 | Acc: 82.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 338/338 [00:50<00:00,  6.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2751 | Acc: 89.82%\nVal Loss: 0.6059 | Acc: 81.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2746 | Acc: 89.63%\nVal Loss: 0.5154 | Acc: 82.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 338/338 [00:50<00:00,  6.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2567 | Acc: 90.54%\nVal Loss: 0.5079 | Acc: 83.41%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2419 | Acc: 90.95%\nVal Loss: 0.6153 | Acc: 81.70%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2341 | Acc: 91.34%\nVal Loss: 0.5823 | Acc: 83.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 338/338 [00:50<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2108 | Acc: 92.10%\nVal Loss: 0.5490 | Acc: 84.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 338/338 [00:50<00:00,  6.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2174 | Acc: 91.69%\nVal Loss: 0.6210 | Acc: 83.96%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30: 100%|██████████| 338/338 [00:50<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2037 | Acc: 92.60%\nVal Loss: 0.6597 | Acc: 83.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1960 | Acc: 92.71%\nVal Loss: 0.6284 | Acc: 82.93%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1868 | Acc: 93.34%\nVal Loss: 0.5839 | Acc: 84.63%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30: 100%|██████████| 338/338 [00:50<00:00,  6.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1948 | Acc: 93.09%\nVal Loss: 0.5783 | Acc: 84.41%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30: 100%|██████████| 338/338 [00:50<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1923 | Acc: 93.10%\nVal Loss: 0.6064 | Acc: 83.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30: 100%|██████████| 338/338 [00:50<00:00,  6.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1868 | Acc: 93.48%\nVal Loss: 0.5420 | Acc: 85.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30: 100%|██████████| 338/338 [00:50<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1709 | Acc: 93.75%\nVal Loss: 0.5246 | Acc: 85.04%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30: 100%|██████████| 338/338 [00:50<00:00,  6.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1618 | Acc: 93.93%\nVal Loss: 0.5290 | Acc: 85.33%\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"'''# 8 Classes\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\n# Define class mapping\nclass_mapping = {\n    \"Seborrheic\": 0,\n    \"Melanocytic\": 1,\n    \"Melanoma\": 2,\n    \"Eczema\": 3,\n    \"Basal_Cell\": 4,\n    \"Psoriasis\": 5,\n    \"Warts_Molluscum\": 6,\n    \"Atopic_Dermatitis\" : 7,\n}\n\n# Preprocess images: resize and normalize\ndef preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"Warning: {image_path} could not be loaded.\")\n        return None\n\n    resized_image = cv2.resize(image, (224, 224))  # Resizing to 224x224\n    img_normalized = resized_image.astype('float32') / 255.0  # Normalize to [0, 1]\n    return img_normalized\n\ndef load_data_from_single_folder(folder):\n    images = []\n    labels = []\n\n    for image_name in os.listdir(folder):\n        image_path = os.path.join(folder, image_name)\n\n        # Check if file is an image\n        if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n            # Extract the label from the filename (before the parentheses)\n            label = image_name.split('(')[0].strip().replace(' ', '_')  # Handle spaces and extract class name\n            \n            if label in class_mapping:\n                label_index = class_mapping[label]  # Map label to integer\n            else:\n                #print(f\"Warning: Label {label} not found in mapping. Skipping image.\")\n                continue\n\n            # Preprocess the image\n            preprocessed_image = preprocess_image(image_path)\n            if preprocessed_image is not None:\n                images.append(preprocessed_image)\n                labels.append(label_index)\n\n    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n    return np.array(images), np.array(labels)\n\n# Custom Dataset class\nclass SkinDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Data augmentation transforms\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Paths for train folder\ntrain_folder = r'/kaggle/working/renamed_train'\n\n# Load data\nX_train, y_train = load_data_from_single_folder(train_folder)\n\n# === Step 6: Class Distribution Analysis ===\nclass_counts = pd.Series(y_train).value_counts()\nclass_names = {v: k for k, v in class_mapping.items()}  # Reverse the mapping\nclass_counts_named = class_counts.rename(index=class_names)\n\nprint(\"\\nClass counts (class names):\")\nprint(class_counts_named)\n\n# === Step 7: Balance Classes to Max Class Size Using Augmentation ===\nmax_class_size = class_counts.max()  # Maximum size among all classes\naugmented_images = []\naugmented_labels = []\n\n# Create augmented images for each class\nfor label in np.unique(y_train):\n    class_images = X_train[y_train == label]\n    current_class_size = class_counts[label]\n    \n    for _ in range(max_class_size - current_class_size):\n        # Select a random image from the class\n        img = class_images[random.randint(0, current_class_size - 1)]\n        \n        # Apply random transformations\n        img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).float()\n        img_tensor = train_transform(img_tensor.numpy().transpose(1, 2, 0))\n        \n        augmented_images.append(img_tensor.numpy().transpose(1, 2, 0))\n        augmented_labels.append(label)\n\n# If augmented images are created, concatenate them with the original data\nif augmented_images:  # Ensure there are augmented images to add\n    X_train = np.concatenate([X_train, np.array(augmented_images)])\n    y_train = np.concatenate([y_train, np.array(augmented_labels)])\n\n# Check new class distribution\nnew_class_counts = pd.Series(y_train).value_counts()\nnew_class_counts_named = new_class_counts.rename(index=class_names)\n\nprint(\"\\nNew class counts after augmentation (class names):\")\nprint(new_class_counts_named)\n\n# Split the dataset into training and validation sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\n# Check new class distribution for train and validation sets\ntrain_class_counts = pd.Series(y_train).value_counts().rename(index=class_names)\ntest_class_counts = pd.Series(y_test).value_counts().rename(index=class_names)\n\nprint(\"\\nClass counts in training set:\")\nprint(train_class_counts)\n\nprint(\"\\nClass counts in test set:\")\nprint(test_class_counts)\n\n# Create datasets\ntrain_dataset = SkinDataset(X_train, y_train, transform=train_transform)\ntest_dataset = SkinDataset(X_test, y_test, transform=test_transform)\n\n# Create data loaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# === Replace with VGG19 Pretrained Model ===\nclass CustomVGG19(nn.Module):\n    def __init__(self, num_classes=len(class_mapping)):\n        super(CustomVGG19, self).__init__()\n        # Load pretrained VGG19\n        self.base_model = models.vgg19(pretrained=True)\n        \n        # Freeze all layers initially\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n            \n        # Unfreeze the last few layers\n        for param in self.base_model.features[-10:].parameters():\n            param.requires_grad = True\n        for param in self.base_model.classifier[-4:].parameters():\n            param.requires_grad = True\n            \n        # Replace the classifier head\n        self.base_model.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        return self.base_model(x)\n\nmodel = CustomVGG19()\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7, verbose=True)\n\n# Early stopping\nearly_stopping_patience = 10\nbest_val_accuracy = 0\nepochs_without_improvement = 0\n\n# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nepochs = 50\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n    \n    train_accuracy = 100 * train_correct / train_total\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    val_accuracy = 100 * val_correct / val_total\n    scheduler.step(val_loss)\n    \n    print(f\"Epoch {epoch+1}/{epochs}\")\n    print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n    print(f\"Val Loss: {val_loss/len(test_loader):.4f}, Val Acc: {val_accuracy:.2f}%\")\n    \n    # Early stopping check\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        epochs_without_improvement = 0\n    else:\n        epochs_without_improvement += 1\n        if epochs_without_improvement >= early_stopping_patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n# Final evaluation\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Confusion matrix for test set\ntest_conf_matrix = confusion_matrix(all_labels, all_preds)\nprint(f\"Test Confusion Matrix:\\n{test_conf_matrix}\")\n\n# Calculate overall accuracy for the test set\ntest_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n\n# Print the first 10 predictions and actual class names from test set\nlabel_to_class = {v: k for k, v in class_mapping.items()}\n\nfor i in range(10):\n    predicted_class = label_to_class[all_preds[i]]\n    actual_class = label_to_class[all_labels[i]]\n    print(f\"Predicted: {predicted_class}, Actual: {actual_class}\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T21:20:03.694686Z","iopub.status.idle":"2025-04-21T21:20:03.695006Z","shell.execute_reply.started":"2025-04-21T21:20:03.694876Z","shell.execute_reply":"2025-04-21T21:20:03.694890Z"}},"outputs":[],"execution_count":null}]}